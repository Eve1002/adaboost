% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/arithmetic.R
\name{adaboost_cv}
\alias{adaboost_cv}
\title{use cross-validation to tune hyperparameter M}
\usage{
adaboost_cv(data, outcome_var, M, folds = 5)
}
\arguments{
\item{data}{we split the original data randomly into 2 datasets, one is train data,}

\item{outcome_var}{response variable or target variable of the dataset}

\item{M}{the number of decision trees included in the ensemble}

\item{folds}{how many folds we use}
}
\value{
average mean squared error across folds
}
\description{
use cross-validation to tune hyperparameter M
}
\examples{
library(rpart)
library(tidyr)
library(tidyverse)
library(gbm)
library(caret)
library(palmerpenguins)
data("penguins")
penguins <- penguins \%>\% drop_na()
set.seed(123)
indices <- sample(1:nrow(penguins), 0.7 * nrow(penguins))
train_data <- penguins[indices, ]
test_data <- penguins[-indices, ]
outcome_variable <- "body_mass_g"
M_values <- c(5, 10, 15, 20, 30, 50, 70)
cv_results <- c()
for(m in 1:length(M_values)){cv_results[m] <- adaboost_cv(train_data, outcome_variable, M=M_values[m], folds = 5)}
best_M <- M_values[which.min(cv_results)]
}
